{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108e97a9-a5d5-4bf8-add4-3f5417b7efe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous output variable. It builds a predictive model in a sequential manner by combining multiple weak learners (usually decision trees) to form a strong learner.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1. What is Gradient Boosting Regression?\n",
    "'''Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous output variable. It builds a predictive model in a sequential manner by combining multiple weak learners (usually decision trees) to form a strong learner.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d29741-6bfe-4827-8a1a-9238a8ad9131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2797\n",
      "R-squared: 0.9921\n"
     ]
    }
   ],
   "source": [
    "#Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and \n",
    "#train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X.squeeze() + np.random.normal(0, 1, X.shape[0])\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.initial_prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize model with mean of y\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        current_prediction = np.full_like(y, self.initial_prediction, dtype=np.float64)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - current_prediction\n",
    "            model = DecisionTreeRegressor(max_depth=3)\n",
    "            model.fit(X, residuals)\n",
    "            prediction = model.predict(X)\n",
    "            current_prediction += self.learning_rate * prediction\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = np.full(X.shape[0], self.initial_prediction, dtype=np.float64)\n",
    "        for model in self.models:\n",
    "            prediction += self.learning_rate * model.predict(X)\n",
    "        return prediction\n",
    "# Create and train the model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1)\n",
    "gb_model.fit(X, y)\n",
    "# Make predictions\n",
    "y_pred = gb_model.predict(X)\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb30155-014e-48b6-a0f5-62f15f076eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Considerations\\nComputational Resources: Grid search can be computationally expensive. Random search can be more efficient, especially with large hyperparameter spaces.\\nCross-Validation: Use cross-validation to ensure that your hyperparameter tuning is robust and not overfitting to the training data.\\nSearch Space: The range of hyperparameters should be chosen based on domain knowledge or preliminary experiments.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the \n",
    "#model. Use grid search or random search to find the best hyperparameters\n",
    "'''Considerations\n",
    "Computational Resources: Grid search can be computationally expensive. Random search can be more efficient, especially with large hyperparameter spaces.\n",
    "Cross-Validation: Use cross-validation to ensure that your hyperparameter tuning is robust and not overfitting to the training data.\n",
    "Search Space: The range of hyperparameters should be chosen based on domain knowledge or preliminary experiments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1b2cad-de02-45df-8207-cdc24146894c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n Gradient Boosting, a weak learner is a model that performs slightly better than random guessing on a given task. Typically, weak learners are simple models that contribute to the overall performance of the boosting algorithm through iterative improvement.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4. What is a weak learner in Gradient Boosting?\n",
    "'''n Gradient Boosting, a weak learner is a model that performs slightly better than random guessing on a given task. Typically, weak learners are simple models that contribute to the overall performance of the boosting algorithm through iterative improvement.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467b1b99-61eb-4288-a56a-60ed1a778133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by combining multiple weak learners in an iterative manner, where each new learner corrects the errors of the previous ones.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "'''The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by combining multiple weak learners in an iterative manner, where each new learner corrects the errors of the previous ones.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8834c3e-584f-4f27-9f44-a938721e7f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gradient Boosting builds an ensemble by iteratively adding weak learners, each focused on correcting the errors of the previous model. This process improves the model's performance incrementally, with each weak learner contributing to the final prediction by addressing residual errors. The combination of all weak learners forms a powerful ensemble that captures complex patterns in the data.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "'''Gradient Boosting builds an ensemble by iteratively adding weak learners, each focused on correcting the errors of the previous model. This process improves the model's performance incrementally, with each weak learner contributing to the final prediction by addressing residual errors. The combination of all weak learners forms a powerful ensemble that captures complex patterns in the data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40be5d45-b4ea-4f5d-bd71-3d533d0123b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Initialize with a simple model.\\nCompute Residuals to identify the errors of the current model.\\nFit a Weak Learner to these residuals.\\nUpdate the Model by adding the weak learner's predictions, scaled by a learning rate.\\nIterate the process to improve the model incrementally.\\nCombine Predictions from all weak learners to form the final ensemble model.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "'''Initialize with a simple model.\n",
    "Compute Residuals to identify the errors of the current model.\n",
    "Fit a Weak Learner to these residuals.\n",
    "Update the Model by adding the weak learner's predictions, scaled by a learning rate.\n",
    "Iterate the process to improve the model incrementally.\n",
    "Combine Predictions from all weak learners to form the final ensemble model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb71bc-869d-476c-b013-0bdfc1010c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
