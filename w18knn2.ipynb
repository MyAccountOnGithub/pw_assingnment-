{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c8fa0c-3ce6-446d-abdd-d1d86496e7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Euclidean distance is the shortest path between source and destination which is a straight line. but Manhattan distance is sum of all the real \\ndistances between source(s) and destination(d) and each distance are always the straight lines \\nThe choice between the distance can affect the preformance of the knn algorithm\\nsensetility of outlier: Euclidean distance is more sensitive to outliers than Manhattan distance. Outliers can disproportionately affect the \\nEuclidean distance calculation since it considers the squared differences\\nDimensionality:In high-dimensional spaces, the curse of dimensionality can affect the performance of distance-based algorithms like KNN. \\nManhattan distance might be more suitable in high-dimensional spaces since it considers distance along each axis independently'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect\n",
    "# the performance of a KNN classifier or regressor?\n",
    "'''Euclidean distance is the shortest path between source and destination which is a straight line. but Manhattan distance is sum of all the real \n",
    "distances between source(s) and destination(d) and each distance are always the straight lines \n",
    "The choice between the distance can affect the preformance of the knn algorithm\n",
    "sensetility of outlier: Euclidean distance is more sensitive to outliers than Manhattan distance. Outliers can disproportionately affect the \n",
    "Euclidean distance calculation since it considers the squared differences\n",
    "Dimensionality:In high-dimensional spaces, the curse of dimensionality can affect the performance of distance-based algorithms like KNN. \n",
    "Manhattan distance might be more suitable in high-dimensional spaces since it considers distance along each axis independently'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21afc24d-502c-45b9-821b-0738ee702f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we can experiment on the k value by giveing the different different value i.e by cross valiadation or we can select by the domain knoweledge of the\\nspecific section'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "'''we can experiment on the k value by giveing the different different value i.e by cross valiadation or we can select by the domain knoweledge of the\n",
    "specific section'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f1dc15-34c7-4402-9256-a839a32f1cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate the distance between the 2 point we can use the euclidiean distance formula or we can use the manhattan distance formula  \\nThe choice of distance metric depends on the nature of the data and the specific problem you are trying to solve.\\nUse Euclidean distance when the features are continuous and have similar scales.\\nUse Manhattan distance when the features have different scales or when you want the algorithm to be more robust to outliers.\\nUse Cosine similarity when you are working with text data or when the magnitude of the vectors is not important.\\nUse Hamming distance when you are working with categorical variables.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance\n",
    "# metric over the other?\n",
    "'''To calculate the distance between the 2 point we can use the euclidiean distance formula or we can use the manhattan distance formula  \n",
    "The choice of distance metric depends on the nature of the data and the specific problem you are trying to solve.\n",
    "Use Euclidean distance when the features are continuous and have similar scales.\n",
    "Use Manhattan distance when the features have different scales or when you want the algorithm to be more robust to outliers.\n",
    "Use Cosine similarity when you are working with text data or when the magnitude of the vectors is not important.\n",
    "Use Hamming distance when you are working with categorical variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e771cf7c-39c7-4f35-9d7d-0a1741eb7d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The parameter like the n_neighbors: that is use to select the neighbor of the data point it is in correct point because when we give the large \\nvalue it does not give the smoother decision\\nleaf_size: This hyperparameter specifies the leaf size passed to the BallTree or KDTree. It can affect the speed of the construction and query, as \\nwell as the memory required to store the tree.\\nweights: This hyperparameter determines how the neighbors are weighted when making predictions. It can be set to 'uniform', where all neighbors are\\nweighted equally, or 'distance', where closer neighbors are given more weight.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go \n",
    "# about tuning these hyperparameters to improve model performance?\n",
    "''' The parameter like the n_neighbors: that is use to select the neighbor of the data point it is in correct point because when we give the large \n",
    "value it does not give the smoother decision\n",
    "leaf_size: This hyperparameter specifies the leaf size passed to the BallTree or KDTree. It can affect the speed of the construction and query, as \n",
    "well as the memory required to store the tree.\n",
    "weights: This hyperparameter determines how the neighbors are weighted when making predictions. It can be set to 'uniform', where all neighbors are\n",
    "weighted equally, or 'distance', where closer neighbors are given more weight.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c60f99-b676-48ee-8b0f-0c47d6ae6dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Size of the data is the affect on the training data set when the large no dataset is their then our model will be perform better and it handle the\\nproblem overfitting, also it increase the computation cost also \\nUse a representative sample: Instead of using the entire dataset for training, you can use a representative sample that captures the essential \\ncharacteristics of the data. This can help reduce the computational cost while maintaining good performance.\\nUse a learning curve: A learning curve plots the model's performance against the size of the training set. By analyzing the learning curve, you can \\ndetermine whether increasing the training set size is likely to improve performance or whether you have already reached a point of diminishing returns.\\nUse data augmentation: Data augmentation involves creating additional training examples by applying transformations to the existing data. This can \\nhelp increase the effective size of the training set without collecting additional data.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size\n",
    "# of the training set?\n",
    "'''Size of the data is the affect on the training data set when the large no dataset is their then our model will be perform better and it handle the\n",
    "problem overfitting, also it increase the computation cost also \n",
    "Use a representative sample: Instead of using the entire dataset for training, you can use a representative sample that captures the essential \n",
    "characteristics of the data. This can help reduce the computational cost while maintaining good performance.\n",
    "Use a learning curve: A learning curve plots the model's performance against the size of the training set. By analyzing the learning curve, you can \n",
    "determine whether increasing the training set size is likely to improve performance or whether you have already reached a point of diminishing returns.\n",
    "Use data augmentation: Data augmentation involves creating additional training examples by applying transformations to the existing data. This can \n",
    "help increase the effective size of the training set without collecting additional data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e773e9-f4af-4644-987f-0109e7583179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Computational complexity: KNN requires storing all training samples, which can be computationally expensive, especially for large datasets. \\nAdditionally, the prediction time increases with the size of the training set.\\nMemory requirements: As KNN stores all training samples, it can require a significant amount of memory, especially for large datasets with\\nhigh-dimensional feature spaces.\\nTo overcome these drawbacks and improve the performance of the KNN model\\nFeature selection: Use feature selection techniques to identify and remove irrelevant or redundant features, which can improve the model's\\nperformance and reduce computational complexity.\\nDimensionality reduction: Use dimensionality reduction techniques such as PCA (Principal Component Analysis) or LDA (Linear Discriminant Analysis) \\nto reduce the number of features and improve the model's efficiency.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance \n",
    "# of the model?\n",
    "'''Computational complexity: KNN requires storing all training samples, which can be computationally expensive, especially for large datasets. \n",
    "Additionally, the prediction time increases with the size of the training set.\n",
    "Memory requirements: As KNN stores all training samples, it can require a significant amount of memory, especially for large datasets with\n",
    "high-dimensional feature spaces.\n",
    "To overcome these drawbacks and improve the performance of the KNN model\n",
    "Feature selection: Use feature selection techniques to identify and remove irrelevant or redundant features, which can improve the model's\n",
    "performance and reduce computational complexity.\n",
    "Dimensionality reduction: Use dimensionality reduction techniques such as PCA (Principal Component Analysis) or LDA (Linear Discriminant Analysis) \n",
    "to reduce the number of features and improve the model's efficiency.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8a1a-5d2a-4eb7-a21b-77d9d6084719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
