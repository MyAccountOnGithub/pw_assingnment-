{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7955396e-c8ee-4325-b505-0f0e2e5b1615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem is a fundamental concept in probability theory and statistics, providing a way to update the probability of a hypothesis based on new evidence. \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1. What is Bayes' theorem?\n",
    "'''Bayes' theorem is a fundamental concept in probability theory and statistics, providing a way to update the probability of a hypothesis based on new evidence. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9948cc28-bce8-4b69-a960-52a7cd2dcce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P(A∣B)= (P(B∣A)⋅P(A))/P(B)\\nP(A∣B) is the conditional probability of event A occurring given that event B has occurred.\\n\\nP(B∣A) is the conditional probability of event \\nB occurring given that event \\nA has occurred.\\nP(A) is the prior probability of event \\nA occurring.\\nP(B) is the marginal probability of event \\nB occurring.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2. What is the formula for Bayes' theorem?\n",
    "'''P(A∣B)= (P(B∣A)⋅P(A))/P(B)\n",
    "P(A∣B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "\n",
    "P(B∣A) is the conditional probability of event \n",
    "B occurring given that event \n",
    "A has occurred.\n",
    "P(A) is the prior probability of event \n",
    "A occurring.\n",
    "P(B) is the marginal probability of event \n",
    "B occurring.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8bc10f-bf51-4e39-b857-0c5db6fb6f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Medical Diagnosis\\nDisease Diagnosis: Bayes' theorem is used to calculate the probability of a disease given a positive test result. For example, if a patient tests positive for a disease, doctors can use Bayes' theorem to update the probability of the patient having the disease based on the test's sensitivity and specificity, as well as the prior probability of the disease in the population.\\n2. Spam Filtering\\nEmail Classification: Bayesian spam filters use Bayes' theorem to classify emails as spam or not spam. The filter calculates the probability that an email is spam based on the presence of certain words and phrases that are common in spam emails.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3. How is Bayes' theorem used in practice?\n",
    "'''Medical Diagnosis\n",
    "Disease Diagnosis: Bayes' theorem is used to calculate the probability of a disease given a positive test result. For example, if a patient tests positive for a disease, doctors can use Bayes' theorem to update the probability of the patient having the disease based on the test's sensitivity and specificity, as well as the prior probability of the disease in the population.\n",
    "2. Spam Filtering\n",
    "Email Classification: Bayesian spam filters use Bayes' theorem to classify emails as spam or not spam. The filter calculates the probability that an email is spam based on the presence of certain words and phrases that are common in spam emails.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9a7c72-98f1-4ae8-90e2-6ed02662198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem is fundamentally based on the concept of conditional probability. Conditional probability is the probability of an event occurring given that another event has already occurred. Bayes' theorem uses this relationship to update the probability of a hypothesis based on new evidence.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "'''Bayes' theorem is fundamentally based on the concept of conditional probability. Conditional probability is the probability of an event occurring given that another event has already occurred. Bayes' theorem uses this relationship to update the probability of a hypothesis based on new evidence.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731289ee-2610-428f-81e0-4c2f43ceb0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Gaussian Naive Bayes\\nUse Case: This is used for continuous data.\\nAssumption: The features follow a normal (Gaussian) distribution.\\nExample Applications: It is often used in situations where the features are continuous, such as in medical data with continuous attributes like age, blood pressure, etc.\\nWhen to choose:\\n\\nWhen your features are continuous and you believe they may follow a Gaussian distribution.\\nWhen you're dealing with numeric data that can be described with mean and variance.\\n2. Multinomial Naive Bayes\\nUse Case: This is used for discrete data.\\nAssumption: The features represent the frequency or count of occurrences.\\nExample Applications: It is commonly used in text classification problems where the features are word counts or term frequencies, such as spam detection or document classification.\\nWhen to choose:\\n\\nWhen your features are discrete and represent counts or frequencies.\\nWhen you're working on text classification problems with bag-of-words or term frequency-inverse document frequency (TF-IDF) representations.\\n3. Bernoulli Naive Bayes\\nUse Case: This is used for binary/boolean data.\\nAssumption: The features are binary (0 or 1), indicating the presence or absence of a feature.\\nExample Applications: It is also used in text classification but specifically when the features are binary indicators of whether a word appears or not in a document.\\nWhen to choose:\\n\\nWhen your features are binary.\\nWhen you are working with binary data representation such as the presence or absence of words in text data.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "'''1. Gaussian Naive Bayes\n",
    "Use Case: This is used for continuous data.\n",
    "Assumption: The features follow a normal (Gaussian) distribution.\n",
    "Example Applications: It is often used in situations where the features are continuous, such as in medical data with continuous attributes like age, blood pressure, etc.\n",
    "When to choose:\n",
    "\n",
    "When your features are continuous and you believe they may follow a Gaussian distribution.\n",
    "When you're dealing with numeric data that can be described with mean and variance.\n",
    "2. Multinomial Naive Bayes\n",
    "Use Case: This is used for discrete data.\n",
    "Assumption: The features represent the frequency or count of occurrences.\n",
    "Example Applications: It is commonly used in text classification problems where the features are word counts or term frequencies, such as spam detection or document classification.\n",
    "When to choose:\n",
    "\n",
    "When your features are discrete and represent counts or frequencies.\n",
    "When you're working on text classification problems with bag-of-words or term frequency-inverse document frequency (TF-IDF) representations.\n",
    "3. Bernoulli Naive Bayes\n",
    "Use Case: This is used for binary/boolean data.\n",
    "Assumption: The features are binary (0 or 1), indicating the presence or absence of a feature.\n",
    "Example Applications: It is also used in text classification but specifically when the features are binary indicators of whether a word appears or not in a document.\n",
    "When to choose:\n",
    "\n",
    "When your features are binary.\n",
    "When you are working with binary data representation such as the presence or absence of words in text data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3facf3-86ea-4f00-8ad1-c394ec95d183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Calculate the Prior Probabilities:\\nAssuming equal prior probabilities:\\n\\nP(A)=P(B)=0.5\\n\\n2. Calculate the Likelihoods:\\nFor class A:\\nP(X1=3∣A)=0.4\\nP(X2=4∣A)≈0.2308\\n\\nFor class B:\\nP(X1=3∣B)= =0.2\\n\\nP(X2=4∣B)=0.3333\\n\\n3. Calculate the Joint Probabilities:\\nFor class A:\\nP(X1=3,X2=4∣A)=0.4×0.2308≈0.0923\\n\\nFor class B:\\nP(X1=3,X2=4∣B)=0.2×0.3333≈0.0667\\n\\n4. Calculate the Posterior Probabilities:\\nFor class A:\\nP(A∣X1=3,X2=4)∝0.0923×0.5=0.0462\\n\\nFor class B:\\nP(B∣X1=3,X2=4)∝0.0667×0.5=0.0333\\n\\n5. Compare and Classify:\\nSince \\nP(A∣X1=3,X2=4)>P(B∣X1=3,X2=4), the Naive Bayes classifier would predict the new instance to belong to class A.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "# each feature value for each class:\n",
    "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "# A      3     3   4    4    3    3    3\n",
    "# B      2     2   1    2    2    2    3\n",
    "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "# to belong to?\n",
    "'''1. Calculate the Prior Probabilities:\n",
    "Assuming equal prior probabilities:\n",
    "\n",
    "P(A)=P(B)=0.5\n",
    "\n",
    "2. Calculate the Likelihoods:\n",
    "For class A:\n",
    "P(X1=3∣A)=0.4\n",
    "P(X2=4∣A)≈0.2308\n",
    "\n",
    "For class B:\n",
    "P(X1=3∣B)= =0.2\n",
    "\n",
    "P(X2=4∣B)=0.3333\n",
    "\n",
    "3. Calculate the Joint Probabilities:\n",
    "For class A:\n",
    "P(X1=3,X2=4∣A)=0.4×0.2308≈0.0923\n",
    "\n",
    "For class B:\n",
    "P(X1=3,X2=4∣B)=0.2×0.3333≈0.0667\n",
    "\n",
    "4. Calculate the Posterior Probabilities:\n",
    "For class A:\n",
    "P(A∣X1=3,X2=4)∝0.0923×0.5=0.0462\n",
    "\n",
    "For class B:\n",
    "P(B∣X1=3,X2=4)∝0.0667×0.5=0.0333\n",
    "\n",
    "5. Compare and Classify:\n",
    "Since \n",
    "P(A∣X1=3,X2=4)>P(B∣X1=3,X2=4), the Naive Bayes classifier would predict the new instance to belong to class A.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac51e3-e797-4081-8878-5dfeaea14ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
