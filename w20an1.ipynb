{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f47e52-0157-4bcb-b8ac-56fadfdb8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anomaly Detection is the process of identifying data points, events, or observations that deviate significantly from the norm or expected behavior within a dataset. These deviations, often referred to as anomalies, outliers, or exceptions, can indicate critical, actionable information.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1. What is anomaly detection and what is its purpose?\n",
    "'''Anomaly Detection is the process of identifying data points, events, or observations that deviate significantly from the norm or expected behavior within a dataset. These deviations, often referred to as anomalies, outliers, or exceptions, can indicate critical, actionable information.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b65eea-8c62-4bbb-a0d4-590a73acea65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Data Quality and Preprocessing\\nChallenge: Poor quality or noisy data can make it difficult to accurately identify anomalies.\\nImpact: Anomalies may be missed or falsely identified due to irrelevant or misleading features.\\nSolution: Implement robust data cleaning and preprocessing techniques to handle missing values, outliers, and noise.\\n2. High Dimensionality\\nChallenge: High-dimensional data can make it difficult to identify anomalies due to the curse of dimensionality.\\nImpact: Distance-based methods may become less effective, and visualizing data for anomaly detection becomes challenging.\\nSolution: Use dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection methods to reduce complexity.\\n3. Class Imbalance\\nChallenge: Anomalies are often rare compared to normal instances, leading to imbalanced datasets.\\nImpact: Standard classification algorithms may struggle to detect anomalies effectively.\\nSolution: Use techniques like resampling (oversampling anomalies, undersampling normal instances), or use anomaly-specific algorithms designed to handle imbalanced data.\\n4. Dynamic Data\\nChallenge: In environments where data characteristics change over time (e.g., evolving patterns in real-time systems), anomalies may shift.\\nImpact: Static models may not adapt to new types of anomalies or changing patterns.\\nSolution: Implement adaptive or incremental learning algorithms that update the model continuously or periodically.\\n5. Definition of Anomalies\\nChallenge: Anomalies can be subjective and context-dependent, making it difficult to define what constitutes an anomaly.\\nImpact: Different definitions of anomalies can lead to varying results and interpretations.\\nSolution: Clearly define the criteria for anomalies based on domain knowledge and context. Consider contextual and domain-specific factors in anomaly detection.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2. What are the key challenges in anomaly detection?\n",
    "'''1. Data Quality and Preprocessing\n",
    "Challenge: Poor quality or noisy data can make it difficult to accurately identify anomalies.\n",
    "Impact: Anomalies may be missed or falsely identified due to irrelevant or misleading features.\n",
    "Solution: Implement robust data cleaning and preprocessing techniques to handle missing values, outliers, and noise.\n",
    "2. High Dimensionality\n",
    "Challenge: High-dimensional data can make it difficult to identify anomalies due to the curse of dimensionality.\n",
    "Impact: Distance-based methods may become less effective, and visualizing data for anomaly detection becomes challenging.\n",
    "Solution: Use dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection methods to reduce complexity.\n",
    "3. Class Imbalance\n",
    "Challenge: Anomalies are often rare compared to normal instances, leading to imbalanced datasets.\n",
    "Impact: Standard classification algorithms may struggle to detect anomalies effectively.\n",
    "Solution: Use techniques like resampling (oversampling anomalies, undersampling normal instances), or use anomaly-specific algorithms designed to handle imbalanced data.\n",
    "4. Dynamic Data\n",
    "Challenge: In environments where data characteristics change over time (e.g., evolving patterns in real-time systems), anomalies may shift.\n",
    "Impact: Static models may not adapt to new types of anomalies or changing patterns.\n",
    "Solution: Implement adaptive or incremental learning algorithms that update the model continuously or periodically.\n",
    "5. Definition of Anomalies\n",
    "Challenge: Anomalies can be subjective and context-dependent, making it difficult to define what constitutes an anomaly.\n",
    "Impact: Different definitions of anomalies can lead to varying results and interpretations.\n",
    "Solution: Clearly define the criteria for anomalies based on domain knowledge and context. Consider contextual and domain-specific factors in anomaly detection.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10315bac-8975-4f05-825e-7aff85702713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unsupervised Anomaly Detection relies on identifying deviations from normal patterns without prior labels and is useful in situations where labeled data is not available.\\nSupervised Anomaly Detection uses labeled examples to train models to identify anomalies and is effective when there is a clear definition of anomalies and sufficient labeled data.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "'''Unsupervised Anomaly Detection relies on identifying deviations from normal patterns without prior labels and is useful in situations where labeled data is not available.\n",
    "Supervised Anomaly Detection uses labeled examples to train models to identify anomalies and is effective when there is a clear definition of anomalies and sufficient labeled data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c327ef1-de62-4789-8931-56c14a7be217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statistical Methods: Based on statistical properties and distributions.\\nDistance-Based Methods: Based on distances to nearest neighbors.\\nClustering-Based Methods: Based on clustering and density.\\nModel-Based Methods: Based on trained models or algorithms.\\nEnsemble Methods: Combining multiple anomaly detection techniques.\\nDeep Learning Methods: Utilizing neural networks and deep learning.\\nHybrid Methods: Combining different approaches for better performance.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4. What are the main categories of anomaly detection algorithms?\n",
    "'''Statistical Methods: Based on statistical properties and distributions.\n",
    "Distance-Based Methods: Based on distances to nearest neighbors.\n",
    "Clustering-Based Methods: Based on clustering and density.\n",
    "Model-Based Methods: Based on trained models or algorithms.\n",
    "Ensemble Methods: Combining multiple anomaly detection techniques.\n",
    "Deep Learning Methods: Utilizing neural networks and deep learning.\n",
    "Hybrid Methods: Combining different approaches for better performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fc21ff-36eb-4875-b917-219fb16562e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normal Points are Dense: Normal points form dense regions; anomalies are sparse.\\nDistance Metric: Distance between points is a meaningful measure.\\nAnomalies are Sparse: Anomalies are relatively rare and scattered.\\nFeature Space Homogeneity: Feature space is assumed to be homogeneous.\\nLocality: Anomalies are detected based on local neighborhood properties.\\nDistance Consistency: Distance measures are assumed to be consistent.\\nDistribution Independence: Anomalies do not affect the normal distribution.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "'''Normal Points are Dense: Normal points form dense regions; anomalies are sparse.\n",
    "Distance Metric: Distance between points is a meaningful measure.\n",
    "Anomalies are Sparse: Anomalies are relatively rare and scattered.\n",
    "Feature Space Homogeneity: Feature space is assumed to be homogeneous.\n",
    "Locality: Anomalies are detected based on local neighborhood properties.\n",
    "Distance Consistency: Distance measures are assumed to be consistent.\n",
    "Distribution Independence: Anomalies do not affect the normal distribution.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228d6fe9-ba52-456e-85f6-db4380156e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Find k-NN: For each point, identify its \\nùëò\\nk-nearest neighbors.\\nCompute Reachability Distance: Calculate the reachability distance for each neighbor.\\nCompute Local Reachability Density (LRD): Estimate the density around each point based on reachability distances.\\nCalculate LOF Score: Compare the LRD of each point to the LRD of its neighbors to determine how much the point‚Äôs density deviates.\\n\\nLOF Score ~ 1: The point has a similar density as its neighbors and is considered normal.\\nLOF Score > 1: The point has a significantly lower density than its neighbors and is considered an outlier.\\nLOF Score < 1: The point has a higher density than its neighbors, which usually does not indicate an anomaly.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6. How does the LOF algorithm compute anomaly scores?\n",
    "'''Find k-NN: For each point, identify its \n",
    "ùëò\n",
    "k-nearest neighbors.\n",
    "Compute Reachability Distance: Calculate the reachability distance for each neighbor.\n",
    "Compute Local Reachability Density (LRD): Estimate the density around each point based on reachability distances.\n",
    "Calculate LOF Score: Compare the LRD of each point to the LRD of its neighbors to determine how much the point‚Äôs density deviates.\n",
    "\n",
    "LOF Score ~ 1: The point has a similar density as its neighbors and is considered normal.\n",
    "LOF Score > 1: The point has a significantly lower density than its neighbors and is considered an outlier.\n",
    "LOF Score < 1: The point has a higher density than its neighbors, which usually does not indicate an anomaly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7bc806-aee2-49c8-924f-6f616b3c422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of Trees (n_estimators): Number of isolation trees in the forest.\\nSubsampling Size (max_samples): Number of samples used to build each tree.\\nMaximum Depth (max_depth): Maximum depth of each tree.\\nContamination (contamination): Proportion of anomalies expected in the data.\\nBootstrap (bootstrap): Whether to use bootstrap sampling.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "'''Number of Trees (n_estimators): Number of isolation trees in the forest.\n",
    "Subsampling Size (max_samples): Number of samples used to build each tree.\n",
    "Maximum Depth (max_depth): Maximum depth of each tree.\n",
    "Contamination (contamination): Proportion of anomalies expected in the data.\n",
    "Bootstrap (bootstrap): Whether to use bootstrap sampling.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d431592c-7430-429d-8141-d18fa41f1615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anomaly Score with k-NN: Given \\nùêæ\\n=\\n10\\nK=10 and only 2 neighbors within radius 0.5, the anomaly score may be computed based on the maximum distance (0.5) or considered high due to insufficient neighbors to meet \\nùêæ\\nK.\\nHandling Insufficient Neighbors: In practice, the algorithm might assign a high anomaly score or use the maximum distance found within the radius as a proxy for further calculations.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "'''Anomaly Score with k-NN: Given \n",
    "ùêæ\n",
    "=\n",
    "10\n",
    "K=10 and only 2 neighbors within radius 0.5, the anomaly score may be computed based on the maximum distance (0.5) or considered high due to insufficient neighbors to meet \n",
    "ùêæ\n",
    "K.\n",
    "Handling Insufficient Neighbors: In practice, the algorithm might assign a high anomaly score or use the maximum distance found within the radius as a proxy for further calculations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895b04bb-ecf0-4df8-a38c-e2e474d05828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anomaly Score: The anomaly score for a data point with an average path length of 5.0, given a dataset of 3000 points and using Isolation Forest with 100 trees, is approximately 0.85. This score suggests that the data point is relatively normal, as scores closer to 1 indicate more normal points and scores closer to 0 indicate more anomalous points'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an \n",
    "#average path length of 5.0 compared to the average path length of the trees?\n",
    "'''Anomaly Score: The anomaly score for a data point with an average path length of 5.0, given a dataset of 3000 points and using Isolation Forest with 100 trees, is approximately 0.85. This score suggests that the data point is relatively normal, as scores closer to 1 indicate more normal points and scores closer to 0 indicate more anomalous points'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16804618-f537-4d72-ab0f-2fe7a19a6c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
