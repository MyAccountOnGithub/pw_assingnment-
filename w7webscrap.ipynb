{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e39533-e83d-492c-9bbe-e89da981c8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web srapping is the process of extracting data from websites.It involve the HTTP request to a special \\nwebsite  and then parsing the response to gather the required data. \\nit used to Extract the data on web page and save them \\nArea of use:\\nPrice Comparison:Websites that offer price comparison services, like those for hotels, flights, or products,\\nuse web scraping to collect pricing data from various sources. By doing this, they can present users with\\nthe most up-to-date prices from different vendors or service providers.\\n\\nJob Aggregators: Websites that provide job listings from various sources often use web scraping to pull job\\npostings from different websites, centralizing them in one place for job seekers.\\n\\nReal Estate: Websites or businesses that provide information about property listings, rental prices, and\\nproperty sales may use web scraping to collect data from various real estate websites. This helps them to \\nanalyze market trends, understand price variations, or offer aggregated listings.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "'''Web srapping is the process of extracting data from websites.It involve the HTTP request to a special \n",
    "website  and then parsing the response to gather the required data. \n",
    "it used to Extract the data on web page and save them \n",
    "Area of use:\n",
    "Price Comparison:Websites that offer price comparison services, like those for hotels, flights, or products,\n",
    "use web scraping to collect pricing data from various sources. By doing this, they can present users with\n",
    "the most up-to-date prices from different vendors or service providers.\n",
    "\n",
    "Job Aggregators: Websites that provide job listings from various sources often use web scraping to pull job\n",
    "postings from different websites, centralizing them in one place for job seekers.\n",
    "\n",
    "Real Estate: Websites or businesses that provide information about property listings, rental prices, and\n",
    "property sales may use web scraping to collect data from various real estate websites. This helps them to \n",
    "analyze market trends, understand price variations, or offer aggregated listings.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2080729-fd4b-4d6c-b14a-86d69c7925a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HTTP Requests:\\n\\nThis is the most basic method where a scraper sends an HTTP request to a website's server and gets the HTML\\nresponse. Libraries like requests in Python can be used for this purpose.\\nHTML Parsing:\\n\\nOnce the HTML content is retrieved, it can be parsed to extract structured information. Libraries like\\nBeautifulSoup in Python are popular for this method.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2. What are the different methods used for Web Scraping?\n",
    "'''HTTP Requests:\n",
    "\n",
    "This is the most basic method where a scraper sends an HTTP request to a website's server and gets the HTML\n",
    "response. Libraries like requests in Python can be used for this purpose.\n",
    "HTML Parsing:\n",
    "\n",
    "Once the HTML content is retrieved, it can be parsed to extract structured information. Libraries like\n",
    "BeautifulSoup in Python are popular for this method.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e48814-84bc-4bf5-9e2f-8e7799cb4fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beautiful Soup is a Python library designed for web scraping purposes to pull the data out of HTML and\\nXML documents. It creates parse trees from page source code that can be used to extract data easily.\\nIt used for:\\nIt arrange the data in proper html format so user can easily read these and perform operation on these '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?\n",
    "'''Beautiful Soup is a Python library designed for web scraping purposes to pull the data out of HTML and\n",
    "XML documents. It creates parse trees from page source code that can be used to extract data easily.\n",
    "It used for:\n",
    "It arrange the data in proper html format so user can easily read these and perform operation on these '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d46606f-2ea0-4e07-a034-47aa917b29f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flask is a lightweight web framework for Python. In the context of a web scraping project, Flask can be \\nused for various purposes, adding a layer of interactivity, user interface, or data presentation to the \\nscraped data. '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?\n",
    "'''Flask is a lightweight web framework for Python. In the context of a web scraping project, Flask can be \n",
    "used for various purposes, adding a layer of interactivity, user interface, or data presentation to the \n",
    "scraped data. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eeb908-39d9-44f7-be51-c2d241966358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "'''AWS CodePipeline:\n",
    "Description: AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service. It allows\n",
    "you to automate the build, test, and deploy phases of your release process.\n",
    "\n",
    "Use: With CodePipeline, you can model the different stages of your software release process, define the \n",
    "actions to be taken at each stage (e.g., code compilation, testing, deployment), and integrate with various \n",
    "AWS and third-party tools. For instance, you can use CodePipeline to automatically deploy your application\n",
    "every time there is a change in the source code in your GitHub repository.\n",
    "\n",
    "AWS Elastic Beanstalk (not Beanstack):\n",
    "\n",
    "Description: AWS Elastic Beanstalk is an orchestration service offered by Amazon Web Services for deploying\n",
    "applications which orchestrates various AWS services, including EC2, S3, and RDS, to make it easier to\n",
    "deploy, scale, and manage applications and services.\n",
    "Use: You simply upload your code (could be a web app or a backend service), and Elastic Beanstalk will take \n",
    "care of the deployment details, from provisioning resources to load balancing and auto-scaling. It supports\n",
    "multiple platforms like Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. For instance, if you have a\n",
    "web application written in Python (using the Flask framework), you can package it and deploy it using \n",
    "Elastic Beanstalk, and the service will ensure it's accessible on the web, handling all the underlying \n",
    "infrastructure and scaling needs.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
